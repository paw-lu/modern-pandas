{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrames need to fit in RAM.\n",
    "You have two solutions for larger datasets:\n",
    "\n",
    "1. Don't use Pandas\n",
    "2. Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Techniques of note</b>\n",
    "    <br><br>\n",
    "    <li><b><code>.nlargest</code></b> to efficiently find the top n values</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task is to find 100 most-common occupations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_parquet(\"data/indiv-10.parq\", columns=[\"occupation\"], engine=\"pyarrow\")\n",
    "\n",
    "most_common = df.occupation.value_counts().nlargest(100)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could only do for the year 2010,\n",
    "because data was too big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do for iteration,\n",
    "we need to rewrite the code\n",
    "in such a say that too much memory is not used at once.\n",
    "\n",
    "1. Create a global `total_counts` Series\n",
    "   that contains the counts from all of the files processed thus far\n",
    "2. Read in a file\n",
    "3. Compute a temporary variable `counts`\n",
    "   with the counts for just one file\n",
    "4. Add temporary `counts` to `total_counts`\n",
    "5. Select the 100 largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(Path(\"data/\").glob(\"indiv-*.parq\"))\n",
    "\n",
    "total_counts = pd.Series()\n",
    "\n",
    "for year in files:\n",
    "    df = pd.read_parquet(year, columns=[\"occupation\"], engine=\"pyarrow\")\n",
    "    counts = df.occupation.value_counts()\n",
    "    total_counts = total_counts.add(counts, fill_value=0)\n",
    "\n",
    "total_counts = total_counts.nlargest(100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet(\"data/indiv-*.parquet\", engine=\"pyarrow\", columns=[\"occupation\"])\n",
    "\n",
    "most_common = df.occupation.value_counts().nlargest(100)\n",
    "most_common.compute().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Techniques of note</b>\n",
    "    <br><br>\n",
    "    <li><b><code>dask.dataframe.visualize</code></b> to view computation graphs</li>\n",
    "    <li><b><code>dask.dataframe.compute</code></b> to fetch data</li>\n",
    "    <li><b><code>dask.distributed.Client</code></b> to execute multiple graphs efficiently—resusing shared computations</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask parallizes Python.\n",
    "It provides APIs for\n",
    "\n",
    "- Arrays\n",
    "- DataFrames\n",
    "- Parallelizing custom algorithms\n",
    "\n",
    "Dask works with **task graphs**—\n",
    "functions to call on data\n",
    "and the relationships between tasks.\n",
    "\n",
    "Dash DataFrame consists of many pandas DataFrames arranged by index.\n",
    "Dash really just coordinates these pandas DataFrames.\n",
    "\n",
    "Dash DataFrames are lazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize graphs\n",
    "df.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can call methods to add tasks to graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = df.occupation.value_counts().nlargest(100)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`most_common` does not hold the answer,\n",
    "it holds a recipe for the answer—\n",
    "a list of steps to take.\n",
    "\n",
    "One way to get an answer out is to call `.compute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point\n",
    "the graph is handed over to a scheduler.\n",
    "\n",
    "`dask.dataframe` and `dask.array`\n",
    "provide familiar APIs for working on large datasets\n",
    "Computations are represented as a task graph.\n",
    "Dask schedulers run task graphs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import seaborn as sns\n",
    "from dask import compute\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(processes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `Client`\n",
    "without providing a scheduler\n",
    "will make a local cluster of threads on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_cols = [\n",
    "    \"cmte_id\",\n",
    "    \"entity_tp\",\n",
    "    \"employer\",\n",
    "    \"occupation\",\n",
    "    \"transaction_dt\",\n",
    "    \"transaction_amt\",\n",
    "]\n",
    "\n",
    "indiv = dd.read_parquet(\"data/indiv-*.parq\", columns=individual_cols, engine=\"pyarrow\")\n",
    "indiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_transaction = indiv.transaction_amt.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which employer's employees donated the most?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_by_employee = indiv.groupby(\"employer\").transaction_amt.sum().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or \"what is the average amount donated per occupation?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_by_occupation = indiv.groupby(\"occupation\").transaction_amt.mean().nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again,\n",
    "Dask is lazy\n",
    "and has yet to compute anything.\n",
    "\n",
    "The three graphs created—\n",
    "`avg_transaction`,\n",
    "`total_by_employee`,\n",
    "and `avg_by_occupation`—\n",
    "have different task graphs\n",
    "but share some common structure.\n",
    "Dask is ample to avoid redundant calculations\n",
    "when `dask.compute` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "avg_transaction, by_employee, by_occupation = compute(\n",
    "    avg_transaction, total_by_employee, avg_by_occupation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use filtering\n",
    "and find the 10 most common occupations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_occupations = (indiv.occupation.value_counts().nlargest(10).index).compute()\n",
    "donations = (\n",
    "    indiv[indiv.occupation.isin(top_occupations)]\n",
    "    .groupby(\"occupation\")\n",
    "    .transaction_amt.agg([\"count\", \"mean\", \"sum\", \"max\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = occupation_avg.sort_values(ascending=False).plot.barh(color=\"k\", width=0.9)\n",
    "lim = ax.get_ylim()\n",
    "ax.vlines(total_avg, *lim, color=\"C1\", linewidth=3)\n",
    "ax.legend([\"Average donation\"])\n",
    "ax.set(xlabel=\"Donation Amount\", title=\"Average Dontation by Occupation\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask has Pandas' time-series support\n",
    "with methods like `resample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = (\n",
    "    indiv[[\"transaction_dt\", \"transaction_amt\"]]\n",
    "    .dropna()\n",
    "    .set_index(\"transaction_dt\")[\"transaction_amt\"]\n",
    "    .resample(\"D\")\n",
    "    .sum()\n",
    ").compute()\n",
    "daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now filter out to just 2011–2016.\n",
    "Dash transitions seamlessly to Pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = daily.loc[\"2011\":\"2016\"]\n",
    "ax = subset.div(1000).plot(figsize=(12, 6))\n",
    "ax.set(\n",
    "    ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\",\n",
    ")\n",
    "sns.despine()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Techniques of note</b>\n",
    "    <br><br>\n",
    "    <li><b><code>dask.dataframe.merge</code></b> To merge Dash DataFrame</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_cols = [\"cmte_id\", \"cmte_nm\", \"cmte_tp\", \"cmte_pty_affiliation\"]\n",
    "cm = dd.read_parquet(\"data/cm-*.parq\", columns=committee_cols).compute()\n",
    "\n",
    "# Some committees change thier name, but the ID stays the same\n",
    "cm = cm.groupby(\"cmte_id\").last()\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv = indiv[\n",
    "    (indiv.transaction_dt >= pd.Timestamp(\"2007-01-01\"))\n",
    "    & (indiv.transaction_dt <= pd.Timestamp(\"2018-01-01\"))\n",
    "]\n",
    "\n",
    "df2 = dd.merge(indiv, cm.reset_index(), on=\"cmte_id\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv = indiv.repartition(npartitions=10)\n",
    "df2 = dd.merge(indiv, cm.reset_index(), on=\"cmte_id\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_donations = (\n",
    "    (df2.groupby([df2.transaction_dt, \"cmte_pty_affiliation\"]).transaction_amt.sum())\n",
    "    .compute()\n",
    "    .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (\n",
    "    party_donations.loc[:, [\"REP\", \"DEM\"]]\n",
    "    .unstack(\"cmte_pty_affiliation\")\n",
    "    .iloc[1:-2]\n",
    "    .rolling(\"30D\")\n",
    "    .mean()\n",
    "    .plot(color=[\"C0\", \"C3\"], figsize=(12, 6), linewidth=3)\n",
    ")\n",
    "sns.despine()\n",
    "ax.set(title=\"Daily Donations (30-D Moving Average)\", xlabel=\"Date\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install by running\n",
    "\n",
    "```sh\n",
    "pip install dask[complete]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent,md"
  },
  "kernelspec": {
   "display_name": "modern-pandas",
   "language": "python",
   "name": "modern-pandas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
